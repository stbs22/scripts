{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__Us3gHGoDSS"
      },
      "source": [
        "**Postgrado UAI**\n",
        "\n",
        "Aprendizaje Reforzado para Sistemas Autónomos - [Prof. Miguel Solis](https://www.miguelsolis.info)\n",
        "\n",
        "08 de noviembre de 2023"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBqU_UY4oMXu"
      },
      "source": [
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install tf-agents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxCStDChoP_w"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.utils import common"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqQ13M5qoW8N"
      },
      "source": [
        "# Configura monitor virtual para visualización de entornos animados\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMIDjTuIoev_"
      },
      "source": [
        "num_iterations = 5000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 100\n",
        "collect_steps_per_iteration = 1\n",
        "replay_buffer_max_length = 100000\n",
        "\n",
        "batch_size = 64\n",
        "learning_rate = 1e-3\n",
        "log_interval = 200\n",
        "\n",
        "num_eval_episodes = 10\n",
        "eval_interval = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3vLTYtNoxKz"
      },
      "source": [
        "env_name = 'CartPole-v0'\n",
        "env = suite_gym.load(env_name)\n",
        "env.reset()\n",
        "PIL.Image.fromarray(env.render())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eL44Lk-o4fV"
      },
      "source": [
        "# Verificación del espacio de estados y espacio de acciones\n",
        "print(\"Espacio de acciones: {}\".format(env.action_space))\n",
        "print(\"Espacio de estados: {}\".format(env.observation_space))\n",
        "\n",
        "time_step = env.reset()\n",
        "print('Instante actual:')\n",
        "print(time_step)\n",
        "\n",
        "action = np.array(1, dtype=np.int32)\n",
        "\n",
        "next_time_step = env.step(action)\n",
        "print('Instante siguiente:')\n",
        "print(next_time_step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhdrf6-CpSUX"
      },
      "source": [
        "# Carga de entornos distintos para entrenamiento y para validación\n",
        "train_py_env = suite_gym.load(env_name)\n",
        "eval_py_env = suite_gym.load(env_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f04rBRwpdsp"
      },
      "source": [
        "# Conversión de arreglos a tensores\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn5Lgs3YpkHG"
      },
      "source": [
        "fc_layer_params = (100, 50)\n",
        "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "\n",
        "def dense_layer(num_units):\n",
        "  return tf.keras.layers.Dense(num_units,\n",
        "activation=tf.keras.activations.relu, kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
        "\n",
        "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
        "q_values_layer = tf.keras.layers.Dense(\n",
        "    num_actions, kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.03, maxval=0.03), bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
        "q_net = sequential.Sequential(dense_layers + [q_values_layer])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJLFr2fcqT8j"
      },
      "source": [
        "# Se instancia un agente con Deep Q-learning\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(train_env.time_step_spec(), train_env.action_spec(), q_network=q_net, optimizer=optimizer,\n",
        "                           td_errors_loss_fn=common.element_wise_squared_loss, train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-eb0t3PuN0x"
      },
      "source": [
        "eval_policy = agent.policy # política principal usada para evaluación y despliegue\n",
        "collect_policy = agent.collect_policy # política secundaria usada para recolección de datos\n",
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec()) # política aleatoria\n",
        "\n",
        "example_environment = tf_py_environment.TFPyEnvironment(suite_gym.load('CartPole-v0'))\n",
        "time_step = example_environment.reset()\n",
        "random_policy.action(time_step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss98Cv1jrog8"
      },
      "source": [
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]\n",
        "\n",
        "def collect_step(environment, policy, buffer):\n",
        "  time_step = environment.current_time_step()\n",
        "  action_step = policy.action(time_step)\n",
        "  next_time_step = environment.step(action_step.action)\n",
        "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "\n",
        "  buffer.add_batch(traj)\n",
        "\n",
        "def collect_data(env, policy, buffer, steps):\n",
        "  for _ in range(steps):\n",
        "    collect_step(env, policy, buffer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPmbpHI7r2ww"
      },
      "source": [
        "# Se crea el buffer de experiencias recolectadas\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=replay_buffer_max_length)\n",
        "\n",
        "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\n",
        "\n",
        "dataset = replay_buffer.as_dataset(num_parallel_calls=3, sample_batch_size=batch_size, num_steps=2).prefetch(3)\n",
        "iterator = iter(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRpP5Suaqqg8"
      },
      "source": [
        "# Entrenamiento\n",
        "agent.train = common.function(agent.train)\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Se evalúa la política inicial\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "  # Se recolectan algunas transiciones\n",
        "  collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
        "  # Se toman algunos de estos datos para entrenar\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = agent.train(experience).loss\n",
        "\n",
        "  step = agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('episodio = {0}: costo = {1}'.format(step, train_loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "    print('episodio = {0}: Retorno promedio = {1}'.format(step, avg_return))\n",
        "    returns.append(avg_return)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxBGjH0TsJSx"
      },
      "source": [
        "# Visualización de desempeño\n",
        "iterations = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(iterations, returns)\n",
        "plt.ylabel('Retorno promedio')\n",
        "plt.xlabel('Episodio')\n",
        "plt.ylim(top=250)\n",
        "plt.grid(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "489HA8kjwEAx"
      },
      "source": [
        "# Función para visualizar videos en python notebook\n",
        "def embed_mp4(filename):\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)\n",
        "\n",
        "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n",
        "  filename = filename + \".mp4\"\n",
        "  with imageio.get_writer(filename, fps=fps) as video:\n",
        "    for _ in range(num_episodes):\n",
        "      time_step = eval_env.reset()\n",
        "      video.append_data(eval_py_env.render())\n",
        "      while not time_step.is_last():\n",
        "        action_step = policy.action(time_step)\n",
        "        time_step = eval_env.step(action_step.action)\n",
        "        video.append_data(eval_py_env.render())\n",
        "  return embed_mp4(filename)\n",
        "\n",
        "create_policy_eval_video(agent.policy, \"pendulo_entrenado\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z74PDZ4_wKHZ"
      },
      "source": [
        "# Comparación con política aleatoria\n",
        "create_policy_eval_video(random_policy, \"pendulo_aleatorio\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}